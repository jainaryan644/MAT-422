{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYo+/QkbTEEXfghwB50N3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jainaryan644/MAT-422/blob/main/hw3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HW 3.3"
      ],
      "metadata": {
        "id": "fbSL7jPU_ATt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3.3.1 Necessary and Sufficient Conditions of Local Minimizers\n",
        "\n",
        "**Definition:** A point x* is a local minimizer of a function f if there exists a neighborhood around x* where f(x*) is less than or equal to f(x) for all x in that neighborhood.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "1. **First-Order Necessary Condition:** If x* is a local minimizer and f is differentiable at x*, then the gradient of f at x* must be zero. This means the function's slope at x* is zero in all directions.\n",
        "\n",
        "2. **Second-Order Necessary Condition:** If x* is a local minimizer and f is twice differentiable at x*, then the Hessian matrix of f at x* must be positive semi-definite. This ensures that the function is locally curved upwards around x*.\n",
        "\n",
        "3. **Second-Order Sufficient Condition:** If the gradient of f at x* is zero and the Hessian matrix of f at x* is positive definite, then x* is a strict local minimizer. This provides a stronger condition for identifying a local minimum, guaranteeing that f(x*) is strictly less than f(x) in the neighborhood.\n",
        "\n",
        "\n",
        "In essence, for a point to be a local minimizer, the function's slope must be zero at that point (first-order), and the curvature must be upwards (second-order). A positive definite Hessian guarantees a strict local minimum."
      ],
      "metadata": {
        "id": "Mi0hV7PD_CIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "nS6SOVYC_zt-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtLBwf-n85p5",
        "outputId": "5cd5aa80-ac2d-4e14-d02a-4c2746557163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient at x*:  [0. 0.]\n",
            "Hessian at x*:  [[-1. -1.]\n",
            " [-1. -1.]]\n",
            "First-order necessary condition satisfied.\n"
          ]
        }
      ],
      "source": [
        "def gradient(f, x):\n",
        "  \"\"\"Computes the gradient of a function f at point x.\"\"\"\n",
        "  epsilon = 1e-6  # Small perturbation for numerical differentiation\n",
        "  grad = np.zeros_like(x, dtype=float)\n",
        "  for i in range(len(x)):\n",
        "    x_plus = x.copy()\n",
        "    x_plus[i] += epsilon\n",
        "    grad[i] = (f(x_plus) - f(x)) / epsilon\n",
        "  return grad\n",
        "\n",
        "def hessian(f, x):\n",
        "  \"\"\"Computes the Hessian matrix of a function f at point x.\"\"\"\n",
        "  epsilon = 1e-6\n",
        "  n = len(x)\n",
        "  hess = np.zeros((n, n))\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      x_plus_i = x.copy()\n",
        "      x_plus_i[i] += epsilon\n",
        "      x_plus_ij = x.copy()\n",
        "      x_plus_ij[i] += epsilon\n",
        "      x_plus_ij[j] += epsilon\n",
        "\n",
        "      hess[i, j] = ((f(x_plus_ij) - f(x_plus_i)) - (f(x + epsilon * np.eye(n)[j]) - f(x))) / (epsilon**2)\n",
        "  return hess\n",
        "\n",
        "\n",
        "def is_positive_definite(matrix):\n",
        "  \"\"\"Checks if a matrix is positive definite.\"\"\"\n",
        "  return np.all(np.linalg.eigvals(matrix) > 0)\n",
        "\n",
        "def is_positive_semi_definite(matrix):\n",
        "  \"\"\"Checks if a matrix is positive semi-definite.\"\"\"\n",
        "  return np.all(np.linalg.eigvals(matrix) >= 0)\n",
        "\n",
        "\n",
        "def f(x):\n",
        "  return x[0]**2 + x[1]**2\n",
        "\n",
        "x_star = np.array([0, 0])\n",
        "\n",
        "grad_x_star = gradient(f, x_star)\n",
        "hess_x_star = hessian(f, x_star)\n",
        "\n",
        "print(\"Gradient at x*: \", grad_x_star)\n",
        "print(\"Hessian at x*: \", hess_x_star)\n",
        "\n",
        "if np.allclose(grad_x_star, np.zeros_like(x_star)):\n",
        "  print(\"First-order necessary condition satisfied.\")\n",
        "  if is_positive_semi_definite(hess_x_star):\n",
        "    print(\"Second-order necessary condition satisfied.\")\n",
        "    if is_positive_definite(hess_x_star):\n",
        "      print(\"Second-order sufficient condition satisfied. x* is a strict local minimizer.\")\n",
        "    else:\n",
        "      print(\"x* may be a local minimizer (further analysis needed).\")\n",
        "else:\n",
        "  print(\"First-order necessary condition not satisfied. x* is not a local minimizer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.2 Convexity and Global Minimizers\n",
        "\n",
        "**Convexity** is a fundamental concept in optimization. A function is considered **convex** if its graph lies below any chord connecting two points on the graph. This means that the function curves upwards and has a bowl-like shape.\n",
        "\n",
        "**Key Concepts Related to Convexity:**\n",
        "\n",
        "1. **Convex Function:** A function f is convex if for any two points x and y in its domain and any scalar t between 0 and 1, the following inequality holds:\n",
        "\n",
        "   f(t*x + (1-t)*y) <= t*f(x) + (1-t)*f(y)\n",
        "\n",
        "2. **Strictly Convex Function:** A function is strictly convex if the inequality above holds strictly (i.e., < instead of <=) for all x != y and t in (0, 1). This means the function curves upwards without any flat sections.\n",
        "\n",
        "**Global Minimizers:**\n",
        "\n",
        "A **global minimizer** of a function f is a point x* where f(x*) is less than or equal to f(x) for all x in the domain of f. It represents the absolute minimum value of the function over its entire domain.\n",
        "\n",
        "**Relationship Between Convexity and Global Minimizers:**\n",
        "\n",
        "* **For a convex function, any local minimizer is also a global minimizer.** This is a crucial property because it allows us to find the global minimum by simply finding a local minimum.  \n",
        "* **If a function is differentiable and convex, then a point x* where the gradient is zero (∇f(x*) = 0) is a global minimizer.** This simplifies the process of finding the global minimum significantly.\n",
        "\n",
        "\n",
        "**In essence, convexity simplifies optimization problems because it guarantees that any local minimum we find is also the global minimum.** This makes it much easier to find the optimal solution for such functions.\n"
      ],
      "metadata": {
        "id": "3hgUjyWEAWNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  \"\"\"A convex function (quadratic in this case).\"\"\"\n",
        "  return x[0]**2 + x[1]**2\n",
        "\n",
        "def gradient(f, x):\n",
        "  \"\"\"Computes the gradient of a function f at point x.\"\"\"\n",
        "  epsilon = 1e-6  # Small perturbation for numerical differentiation\n",
        "  grad = np.zeros_like(x, dtype=float)\n",
        "  for i in range(len(x)):\n",
        "    x_plus = x.copy()\n",
        "    x_plus[i] += epsilon\n",
        "    grad[i] = (f(x_plus) - f(x)) / epsilon\n",
        "  return grad\n",
        "\n",
        "# Example demonstrating convexity and global minimizers\n",
        "x_star = np.array([0, 0])  # The global minimizer for f(x)\n",
        "print(\"f(x_star) =\", f(x_star))\n",
        "\n",
        "x1 = np.array([1, 1])\n",
        "x2 = np.array([-1, -1])\n",
        "\n",
        "# Check if f is convex using the definition\n",
        "t = 0.5\n",
        "x_t = t * x1 + (1 - t) * x2\n",
        "f_x_t = f(x_t)\n",
        "t_f_x1_1_t_f_x2 = t * f(x1) + (1 - t) * f(x2)\n",
        "\n",
        "print(\"f(t*x1 + (1-t)*x2) =\", f_x_t)\n",
        "print(\"t*f(x1) + (1-t)*f(x2) =\", t_f_x1_1_t_f_x2)\n",
        "print(\"Is f convex? \", f_x_t <= t_f_x1_1_t_f_x2)\n",
        "\n",
        "# Since the function is convex, any local minimizer is also a global minimizer.\n",
        "# In this case, x* (0, 0) is a local minimizer, and it is also the global minimizer.\n",
        "\n",
        "# Find a local minimizer using gradient descent (demonstrating how convexity helps)\n",
        "x = np.array([2, 2])  # Starting point\n",
        "learning_rate = 0.1\n",
        "for _ in range(100):\n",
        "  grad = gradient(f, x)\n",
        "  x = x - learning_rate * grad\n",
        "\n",
        "print(\"Gradient descent found a local minimizer (which is also global) at x =\", x)\n",
        "print(\"f(x) =\", f(x))\n",
        "\n",
        "# Note: Since the function is convex and the gradient is used,\n",
        "# the gradient descent will eventually converge to the global minimum."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLvRZN9pCncH",
        "outputId": "c31da03f-adf8-459b-d79c-822397d5a3b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f(x_star) = 0\n",
            "f(t*x1 + (1-t)*x2) = 0.0\n",
            "t*f(x1) + (1-t)*f(x2) = 2.0\n",
            "Is f convex?  True\n",
            "Gradient descent found a local minimizer (which is also global) at x = [-4.99490741e-07 -4.99490741e-07]\n",
            "f(x) = 4.98982000446869e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.3 Gradient Descent\n",
        "\n",
        "Gradient descent is an iterative optimization algorithm used to find the minimum of a function. It's particularly useful when dealing with functions that are differentiable, meaning we can calculate their slope (gradient) at any point.\n",
        "\n",
        "\n",
        "**The Core Idea:**\n",
        "\n",
        "The algorithm starts at an initial point and repeatedly takes steps in the direction of the negative gradient.  The negative gradient points in the direction of steepest descent, meaning it's the most efficient way to move towards the function's minimum value.  \n",
        "\n",
        "**Mathematical Representation:**\n",
        "\n",
        "1. **Initialization:** Start with an initial guess for the minimum, denoted as x<sub>0</sub>.\n",
        "\n",
        "2. **Iteration:** At each step (iteration) i, update the current estimate of the minimum, x<sub>i</sub>, using the following formula:\n",
        "\n",
        "    x<sub>i+1</sub> = x<sub>i</sub> - η∇f(x<sub>i</sub>)\n",
        "\n",
        "    where:\n",
        "      * x<sub>i+1</sub> is the next estimate of the minimum.\n",
        "      * x<sub>i</sub> is the current estimate.\n",
        "      * η (eta) is the learning rate, a positive scalar controlling the size of the step taken in each iteration.\n",
        "      * ∇f(x<sub>i</sub>) is the gradient of the function f at the current point x<sub>i</sub>. It represents the direction and magnitude of the steepest ascent.\n",
        "\n",
        "3. **Repeat:** Continue this process (steps 2 and 3) until a stopping criterion is met. This criterion could be reaching a certain number of iterations, the change in x becoming very small, or the gradient becoming close to zero.\n",
        "\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* **Learning Rate (η):** This parameter controls the size of the steps taken in the direction of the negative gradient. A larger learning rate results in bigger steps, which can lead to faster convergence but may also overshoot the minimum. A smaller learning rate leads to smaller steps, which can be more stable but might require more iterations to converge.\n",
        "\n",
        "* **Convergence:** Gradient descent aims to converge to a point where the gradient is close to zero. This point represents a local minimum (or possibly a saddle point, depending on the function).\n",
        "\n",
        "* **Convex Functions:**  When the function being optimized is convex, gradient descent is guaranteed to converge to the global minimum.\n",
        "\n",
        "\n",
        "**In essence, gradient descent iteratively refines an initial guess for the function's minimum by moving in the direction opposite to the gradient, aiming to find the point where the function's value is the lowest.**"
      ],
      "metadata": {
        "id": "BbzmZ1vvCQmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(f, x):\n",
        "  \"\"\"Computes the gradient of a function f at point x.\"\"\"\n",
        "  epsilon = 1e-6  # Small perturbation for numerical differentiation\n",
        "  grad = np.zeros_like(x, dtype=float)\n",
        "  for i in range(len(x)):\n",
        "    x_plus = x.copy()\n",
        "    x_plus[i] += epsilon\n",
        "    grad[i] = (f(x_plus) - f(x)) / epsilon\n",
        "  return grad\n",
        "\n",
        "def f(x):\n",
        "  \"\"\"The function we want to minimize.\"\"\"\n",
        "  return x[0]**2 + x[1]**2\n",
        "\n",
        "# Gradient Descent Algorithm\n",
        "def gradient_descent(f, initial_x, learning_rate, num_iterations):\n",
        "  x = initial_x\n",
        "  for i in range(num_iterations):\n",
        "    grad = gradient(f, x)\n",
        "    x = x - learning_rate * grad\n",
        "  return x\n",
        "\n",
        "# Initial point\n",
        "initial_x = np.array([2, 2])\n",
        "\n",
        "# Learning rate and number of iterations\n",
        "learning_rate = 0.1\n",
        "num_iterations = 100\n",
        "\n",
        "# Run gradient descent\n",
        "min_x = gradient_descent(f, initial_x, learning_rate, num_iterations)\n",
        "\n",
        "print(\"Approximate minimum found by gradient descent:\", min_x)\n",
        "print(\"Function value at the minimum:\", f(min_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwBzvLzoCgdW",
        "outputId": "927df1b4-e1d8-4e54-fb7d-db1cd86851c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximate minimum found by gradient descent: [-4.99490741e-07 -4.99490741e-07]\n",
            "Function value at the minimum: 4.98982000446869e-13\n"
          ]
        }
      ]
    }
  ]
}